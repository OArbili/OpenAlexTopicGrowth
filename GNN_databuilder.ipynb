{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b77736fa-7247-43bd-b77c-095ed00896eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Graph Structure Analysis:\n",
      "Total nodes: 3402\n",
      "Total edges: 30143\n",
      "\n",
      "Top 5 Most Connected Topics:\n",
      "- Gut microbiota and health\n",
      "  Connections: 158\n",
      "  Field: Biochemistry, Genetics and Molecular Biology\n",
      "  Subfield: Molecular Biology\n",
      "- Artificial Intelligence in Healthcare and Education\n",
      "  Connections: 152\n",
      "  Field: Medicine\n",
      "  Subfield: Health Informatics\n",
      "- Machine Learning in Materials Science\n",
      "  Connections: 141\n",
      "  Field: Materials Science\n",
      "  Subfield: Materials Chemistry\n",
      "- Epigenetics and DNA Methylation\n",
      "  Connections: 129\n",
      "  Field: Biochemistry, Genetics and Molecular Biology\n",
      "  Subfield: Molecular Biology\n",
      "- Single-cell and spatial transcriptomics\n",
      "  Connections: 121\n",
      "  Field: Biochemistry, Genetics and Molecular Biology\n",
      "  Subfield: Molecular Biology\n",
      "\n",
      "Top 5 Strongest Topic Collaborations:\n",
      "- Advancements in Battery Materials <-> Advanced Battery Materials and Technologies\n",
      "  Weight: 3999.097\n",
      "  Joint Papers: 239.0\n",
      "  Fields: Engineering <-> Engineering\n",
      "- Advancements in Battery Materials <-> Advanced Battery Technologies Research\n",
      "  Weight: 2358.918\n",
      "  Joint Papers: 148.0\n",
      "  Fields: Engineering <-> Engineering\n",
      "- Advanced Battery Materials and Technologies <-> Advanced Battery Technologies Research\n",
      "  Weight: 2192.910\n",
      "  Joint Papers: 132.0\n",
      "  Fields: Engineering <-> Engineering\n",
      "- Quantum Information and Cryptography <-> Quantum Computing Algorithms and Architecture\n",
      "  Weight: 1948.060\n",
      "  Joint Papers: 128.0\n",
      "  Fields: Computer Science <-> Computer Science\n",
      "- Galaxies: Formation, Evolution, Phenomena <-> Astronomy and Astrophysical Research\n",
      "  Weight: 1816.854\n",
      "  Joint Papers: 107.0\n",
      "  Fields: Physics and Astronomy <-> Physics and Astronomy\n",
      "\n",
      "Top 5 Field-Level Collaborations:\n",
      "- Biochemistry, Genetics and Molecular Biology <-> Medicine: 2049 connections\n",
      "- Engineering <-> Materials Science: 839 connections\n",
      "- Computer Science <-> Engineering: 610 connections\n",
      "- Immunology and Microbiology <-> Medicine: 547 connections\n",
      "- Medicine <-> Neuroscience: 516 connections\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Loaded 1000 papers from openalex_topics_results/highly_cited_articles_2024_20250221_222754.json\n",
      "INFO:root:Loaded 3000 papers from openalex_topics_results/highly_cited_articles_2024_20250221_222818.json\n",
      "INFO:root:Loaded 6000 papers from openalex_topics_results/highly_cited_articles_2024_20250221_222842.json\n",
      "INFO:root:Loaded 10000 papers from openalex_topics_results/highly_cited_articles_2024_20250221_222907.json\n",
      "INFO:root:Loaded 15000 papers from openalex_topics_results/highly_cited_articles_2024_20250221_222933.json\n",
      "INFO:root:Loaded 21000 papers from openalex_topics_results/highly_cited_articles_2024_20250221_223000.json\n",
      "INFO:root:Loaded 28000 papers from openalex_topics_results/highly_cited_articles_2024_20250221_223026.json\n",
      "INFO:root:Loaded 36000 papers from openalex_topics_results/highly_cited_articles_2024_20250221_223052.json\n",
      "INFO:root:Loaded 45000 papers from openalex_topics_results/highly_cited_articles_2024_20250221_223119.json\n",
      "INFO:root:Loaded 55000 papers from openalex_topics_results/highly_cited_articles_2024_20250221_223149.json\n",
      "INFO:root:Loaded 66000 papers from openalex_topics_results/highly_cited_articles_2024_20250221_223218.json\n",
      "INFO:root:Loaded 78000 papers from openalex_topics_results/highly_cited_articles_2024_20250221_223247.json\n",
      "INFO:root:Loaded 91000 papers from openalex_topics_results/highly_cited_articles_2024_20250221_223315.json\n",
      "INFO:root:Loaded 105000 papers from openalex_topics_results/highly_cited_articles_2024_20250221_223343.json\n",
      "INFO:root:Loaded 120000 papers from openalex_topics_results/highly_cited_articles_2024_20250221_223412.json\n",
      "INFO:root:Loaded 136000 papers from openalex_topics_results/highly_cited_articles_2024_20250221_223441.json\n",
      "INFO:root:Loaded 153000 papers from openalex_topics_results/highly_cited_articles_2024_20250221_223512.json\n",
      "INFO:root:Loaded 171000 papers from openalex_topics_results/highly_cited_articles_2024_20250221_223543.json\n",
      "INFO:root:Loaded 190000 papers from openalex_topics_results/highly_cited_articles_2024_20250221_223612.json\n",
      "INFO:root:Loaded 210000 papers from openalex_topics_results/highly_cited_articles_2024_20250221_223643.json\n",
      "INFO:root:Loaded 231000 papers from openalex_topics_results/highly_cited_articles_2024_20250221_223714.json\n",
      "INFO:root:Loaded 253000 papers from openalex_topics_results/highly_cited_articles_2024_20250221_223746.json\n",
      "INFO:root:Loaded 276000 papers from openalex_topics_results/highly_cited_articles_2024_20250221_223815.json\n",
      "INFO:root:Loaded 300000 papers from openalex_topics_results/highly_cited_articles_2024_20250221_223847.json\n",
      "INFO:root:Loaded 325000 papers from openalex_topics_results/highly_cited_articles_2024_20250221_223919.json\n",
      "INFO:root:Loaded 351000 papers from openalex_topics_results/highly_cited_articles_2024_20250221_223951.json\n",
      "INFO:root:Loaded 378000 papers from openalex_topics_results/highly_cited_articles_2024_20250221_224025.json\n",
      "INFO:root:Loaded 405813 papers from openalex_topics_results/highly_cited_articles_2024_20250221_224056.json\n",
      "INFO:root:Total papers loaded: 405813\n",
      "2025-02-22 17:33:53,343 - INFO - Starting to process 405813 papers\n",
      "INFO:__main__:Starting to process 405813 papers\n",
      "2025-02-22 17:33:58,362 - INFO - Found 4 domains, 26 fields, 246 subfields, 3402 topics\n",
      "INFO:__main__:Found 4 domains, 26 fields, 246 subfields, 3402 topics\n",
      "2025-02-22 17:33:58,363 - INFO - Building GNN data with dimensions: 3402 topics, 4 domains, 26 fields, 246 subfields\n",
      "INFO:__main__:Building GNN data with dimensions: 3402 topics, 4 domains, 26 fields, 246 subfields\n",
      "2025-02-22 17:34:14,127 - INFO - Processed 10000 papers...\n",
      "INFO:__main__:Processed 10000 papers...\n",
      "2025-02-22 17:34:14,202 - INFO - Processed 20000 papers...\n",
      "INFO:__main__:Processed 20000 papers...\n",
      "2025-02-22 17:34:14,277 - INFO - Processed 30000 papers...\n",
      "INFO:__main__:Processed 30000 papers...\n",
      "2025-02-22 17:34:14,351 - INFO - Processed 40000 papers...\n",
      "INFO:__main__:Processed 40000 papers...\n",
      "2025-02-22 17:34:14,408 - INFO - Processed 50000 papers...\n",
      "INFO:__main__:Processed 50000 papers...\n",
      "2025-02-22 17:34:14,459 - INFO - Processed 60000 papers...\n",
      "INFO:__main__:Processed 60000 papers...\n",
      "2025-02-22 17:34:14,510 - INFO - Processed 70000 papers...\n",
      "INFO:__main__:Processed 70000 papers...\n",
      "2025-02-22 17:34:14,562 - INFO - Processed 80000 papers...\n",
      "INFO:__main__:Processed 80000 papers...\n",
      "2025-02-22 17:34:14,630 - INFO - Processed 90000 papers...\n",
      "INFO:__main__:Processed 90000 papers...\n",
      "2025-02-22 17:34:14,706 - INFO - Processed 100000 papers...\n",
      "INFO:__main__:Processed 100000 papers...\n",
      "2025-02-22 17:34:14,781 - INFO - Processed 110000 papers...\n",
      "INFO:__main__:Processed 110000 papers...\n",
      "2025-02-22 17:36:11,081 - INFO - Processed 120000 papers...\n",
      "INFO:__main__:Processed 120000 papers...\n",
      "2025-02-22 17:36:11,167 - INFO - Processed 130000 papers...\n",
      "INFO:__main__:Processed 130000 papers...\n",
      "2025-02-22 17:36:11,245 - INFO - Processed 140000 papers...\n",
      "INFO:__main__:Processed 140000 papers...\n",
      "2025-02-22 17:36:11,328 - INFO - Processed 150000 papers...\n",
      "INFO:__main__:Processed 150000 papers...\n",
      "2025-02-22 17:36:11,405 - INFO - Processed 160000 papers...\n",
      "INFO:__main__:Processed 160000 papers...\n",
      "2025-02-22 17:36:11,482 - INFO - Processed 170000 papers...\n",
      "INFO:__main__:Processed 170000 papers...\n",
      "2025-02-22 17:36:11,564 - INFO - Processed 180000 papers...\n",
      "INFO:__main__:Processed 180000 papers...\n",
      "2025-02-22 17:36:11,683 - INFO - Processed 190000 papers...\n",
      "INFO:__main__:Processed 190000 papers...\n",
      "2025-02-22 17:36:11,800 - INFO - Processed 200000 papers...\n",
      "INFO:__main__:Processed 200000 papers...\n",
      "2025-02-22 17:36:11,922 - INFO - Processed 210000 papers...\n",
      "INFO:__main__:Processed 210000 papers...\n",
      "2025-02-22 17:36:12,034 - INFO - Processed 220000 papers...\n",
      "INFO:__main__:Processed 220000 papers...\n",
      "2025-02-22 17:36:12,142 - INFO - Processed 230000 papers...\n",
      "INFO:__main__:Processed 230000 papers...\n",
      "2025-02-22 17:36:12,227 - INFO - Processed 240000 papers...\n",
      "INFO:__main__:Processed 240000 papers...\n",
      "2025-02-22 17:36:12,298 - INFO - Processed 250000 papers...\n",
      "INFO:__main__:Processed 250000 papers...\n",
      "2025-02-22 17:36:12,371 - INFO - Processed 260000 papers...\n",
      "INFO:__main__:Processed 260000 papers...\n",
      "2025-02-22 17:36:12,475 - INFO - Processed 270000 papers...\n",
      "INFO:__main__:Processed 270000 papers...\n",
      "2025-02-22 17:36:12,584 - INFO - Processed 280000 papers...\n",
      "INFO:__main__:Processed 280000 papers...\n",
      "2025-02-22 17:36:12,688 - INFO - Processed 290000 papers...\n",
      "INFO:__main__:Processed 290000 papers...\n",
      "2025-02-22 17:36:12,799 - INFO - Processed 300000 papers...\n",
      "INFO:__main__:Processed 300000 papers...\n",
      "2025-02-22 17:36:12,904 - INFO - Processed 310000 papers...\n",
      "INFO:__main__:Processed 310000 papers...\n",
      "2025-02-22 17:36:13,012 - INFO - Processed 320000 papers...\n",
      "INFO:__main__:Processed 320000 papers...\n",
      "2025-02-22 17:36:13,106 - INFO - Processed 330000 papers...\n",
      "INFO:__main__:Processed 330000 papers...\n",
      "2025-02-22 17:36:13,210 - INFO - Processed 340000 papers...\n",
      "INFO:__main__:Processed 340000 papers...\n",
      "2025-02-22 17:36:13,316 - INFO - Processed 350000 papers...\n",
      "INFO:__main__:Processed 350000 papers...\n",
      "2025-02-22 17:36:13,393 - INFO - Processed 360000 papers...\n",
      "INFO:__main__:Processed 360000 papers...\n",
      "2025-02-22 17:36:13,479 - INFO - Processed 370000 papers...\n",
      "INFO:__main__:Processed 370000 papers...\n",
      "2025-02-22 17:36:13,544 - INFO - Processed 380000 papers...\n",
      "INFO:__main__:Processed 380000 papers...\n",
      "2025-02-22 17:36:13,617 - INFO - Processed 390000 papers...\n",
      "INFO:__main__:Processed 390000 papers...\n",
      "2025-02-22 17:36:13,680 - INFO - Processed 400000 papers...\n",
      "INFO:__main__:Processed 400000 papers...\n",
      "2025-02-22 17:36:13,721 - INFO - Found 30143 unique topic connections\n",
      "INFO:__main__:Found 30143 unique topic connections\n",
      "2025-02-22 17:36:13,721 - INFO - Building node features...\n",
      "INFO:__main__:Building node features...\n",
      "2025-02-22 17:36:13,764 - INFO - Building edge index and features...\n",
      "INFO:__main__:Building edge index and features...\n",
      "2025-02-22 17:36:13,818 - INFO - Created graph with 3402 nodes and 30143 edges\n",
      "INFO:__main__:Created graph with 3402 nodes and 30143 edges\n",
      "2025-02-22 17:36:14,038 - INFO - Saved GNN data to processed_data/gnn_data.pkl and processed_data/gnn_data.npz\n",
      "INFO:__main__:Saved GNN data to processed_data/gnn_data.pkl and processed_data/gnn_data.npz\n",
      "ERROR:root:Error in processing: name 'analyze_graph_structure' is not defined\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "GNN Data Summary:\n",
      "Number of nodes (topics): 3402\n",
      "Number of edges: 30143\n",
      "Node feature dimensions: 279\n",
      "Edge feature dimensions: 6\n",
      "\n",
      "Feature Breakdowns:\n",
      "Node features:\n",
      "- papers: mean = 346.31\n",
      "- avg_score: mean = 0.97\n",
      "- total_collabs: mean = 679.11\n",
      "\n",
      "Edge features:\n",
      "- weight: mean = 37.47\n",
      "- normalized_weight: mean = 13.51\n",
      "- joint_papers: mean = 2.63\n",
      "- same_domain: mean = 0.68\n",
      "- same_field: mean = 0.42\n",
      "- same_subfield: mean = 0.14\n",
      "\n",
      "GNN Data Summary:\n",
      "Number of nodes (topics): 3402\n",
      "Number of edges: 2\n",
      "Node feature dimensions: 279\n",
      "Edge feature dimensions: 6\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'analyze_graph_structure' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 534\u001b[0m\n\u001b[1;32m    531\u001b[0m         saved_data \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[1;32m    532\u001b[0m         gnn_data \u001b[38;5;241m=\u001b[39m saved_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgnn_data\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m--> 534\u001b[0m     \u001b[43manalyze_graph_structure\u001b[49m(gnn_data)\n\u001b[1;32m    536\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    537\u001b[0m     logging\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError in processing: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'analyze_graph_structure' is not defined"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import glob\n",
    "import os\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "import logging\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Optional, Tuple, Set\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "class GNNDataBuilder:\n",
    "    \"\"\"\n",
    "    Builds and saves GNN-ready data structure from academic papers.\n",
    "    Node features include topic statistics and one-hot encoded hierarchical information.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 min_topic_score: float = 0.3,\n",
    "                 data_folder: str = \"raw_data\",\n",
    "                 processed_folder: str = \"processed_data\"):\n",
    "        self.min_topic_score = min_topic_score\n",
    "        self.data_folder = data_folder\n",
    "        self.processed_folder = processed_folder\n",
    "        self.logger = self._setup_logger()\n",
    "        \n",
    "        # Create directories\n",
    "        os.makedirs(processed_folder, exist_ok=True)\n",
    "        \n",
    "        # Mapping dictionaries for encoding\n",
    "        self.topic_to_idx = {}\n",
    "        self.domain_to_idx = {}\n",
    "        self.field_to_idx = {}\n",
    "        self.subfield_to_idx = {}\n",
    "        \n",
    "        # Reverse mappings for decoding\n",
    "        self.idx_to_topic = {}\n",
    "        self.idx_to_domain = {}\n",
    "        self.idx_to_field = {}\n",
    "        self.idx_to_subfield = {}\n",
    "        \n",
    "        self.metadata = {\n",
    "            'min_topic_score': min_topic_score,\n",
    "            'version': '1.0',\n",
    "            'timestamp': None,\n",
    "            'stats': {\n",
    "                'num_topics': 0,\n",
    "                'num_domains': 0,\n",
    "                'num_fields': 0,\n",
    "                'num_subfields': 0\n",
    "            }\n",
    "        }\n",
    "\n",
    "    def build_category_indices(self, papers: List[dict]):\n",
    "        \"\"\"Build indices for all categories (domains, fields, subfields, topics)\"\"\"\n",
    "        self.logger.info(f\"Starting to process {len(papers)} papers\")\n",
    "        \n",
    "        # Collect unique categories\n",
    "        domains = set()\n",
    "        fields = set()\n",
    "        subfields = set()\n",
    "        topics = {}  # topic_id -> topic_info\n",
    "        \n",
    "        for paper in papers:\n",
    "            for topic in paper.get('topics', []):\n",
    "                if topic.get('score', 0) >= self.min_topic_score:\n",
    "                    # Extract hierarchical information\n",
    "                    domain_info = topic.get('domain', {})\n",
    "                    field_info = topic.get('field', {})\n",
    "                    subfield_info = topic.get('subfield', {})\n",
    "                    \n",
    "                    # Store complete URLs as identifiers\n",
    "                    topic_id = topic['id']  # This is the full URL\n",
    "                    domain_id = domain_info.get('id', '')\n",
    "                    field_id = field_info.get('id', '')\n",
    "                    subfield_id = subfield_info.get('id', '')\n",
    "                    \n",
    "                    domains.add((domain_id, domain_info.get('display_name', '')))\n",
    "                    fields.add((field_id, field_info.get('display_name', '')))\n",
    "                    subfields.add((subfield_id, subfield_info.get('display_name', '')))\n",
    "                    \n",
    "                    topics[topic_id] = {\n",
    "                        'display_name': topic.get('display_name', ''),\n",
    "                        'domain': domain_info.get('display_name', ''),\n",
    "                        'field': field_info.get('display_name', ''),\n",
    "                        'subfield': subfield_info.get('display_name', '')\n",
    "                    }\n",
    "            \n",
    "        # Create indices using the full URLs\n",
    "        self.domain_to_idx = {id_: idx for idx, (id_, _) in enumerate(sorted(domains))}\n",
    "        self.field_to_idx = {id_: idx for idx, (id_, _) in enumerate(sorted(fields))}\n",
    "        self.subfield_to_idx = {id_: idx for idx, (id_, _) in enumerate(sorted(subfields))}\n",
    "        self.topic_to_idx = {id_: idx for idx, id_ in enumerate(sorted(topics.keys()))}\n",
    "        \n",
    "        # Create reverse mappings\n",
    "        self.idx_to_domain = {idx: name for (_, name), idx in zip(sorted(domains), range(len(domains)))}\n",
    "        self.idx_to_field = {idx: name for (_, name), idx in zip(sorted(fields), range(len(fields)))}\n",
    "        self.idx_to_subfield = {idx: name for (_, name), idx in zip(sorted(subfields), range(len(subfields)))}\n",
    "        self.idx_to_topic = {idx: {'id': id_, 'info': topics[id_]} \n",
    "                            for id_, idx in self.topic_to_idx.items()}\n",
    "        \n",
    "        # Update metadata\n",
    "        self.metadata['stats'].update({\n",
    "            'num_domains': len(domains),\n",
    "            'num_fields': len(fields),\n",
    "            'num_subfields': len(subfields),\n",
    "            'num_topics': len(topics)\n",
    "        })\n",
    "        \n",
    "        self.logger.info(f\"Found {len(domains)} domains, {len(fields)} fields, \"\n",
    "                        f\"{len(subfields)} subfields, {len(topics)} topics\")\n",
    "    \n",
    "    def _setup_logger(self):\n",
    "        \"\"\"Setup logging configuration\"\"\"\n",
    "        logger = logging.getLogger(__name__)\n",
    "        logger.setLevel(logging.INFO)\n",
    "        if not logger.handlers:\n",
    "            handler = logging.StreamHandler()\n",
    "            formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "            handler.setFormatter(formatter)\n",
    "            logger.addHandler(handler)\n",
    "        return logger\n",
    "\n",
    "    \n",
    "    def build_gnn_data(self, papers: List[dict]) -> Dict:\n",
    "        \"\"\"\n",
    "        Build complete GNN-ready data structure from papers\n",
    "        \n",
    "        Args:\n",
    "            papers (List[dict]): List of papers with topic information\n",
    "            \n",
    "        Returns:\n",
    "            Dict: Complete GNN data structure containing:\n",
    "                - node_features: [num_topics x num_features] matrix \n",
    "                - edge_index: [2 x num_edges] matrix\n",
    "                - edge_features: [num_edges x 6] matrix\n",
    "                - metadata and mappings\n",
    "        \"\"\"\n",
    "        # Get dimensions\n",
    "        num_topics = len(self.topic_to_idx)\n",
    "        num_domains = len(self.domain_to_idx)\n",
    "        num_fields = len(self.field_to_idx)\n",
    "        num_subfields = len(self.subfield_to_idx)\n",
    "        \n",
    "        self.logger.info(f\"Building GNN data with dimensions: \"\n",
    "                        f\"{num_topics} topics, {num_domains} domains, \"\n",
    "                        f\"{num_fields} fields, {num_subfields} subfields\")\n",
    "        \n",
    "        # Initialize node features\n",
    "        total_features = 3 + num_domains + num_fields + num_subfields\n",
    "        node_features = np.zeros((num_topics, total_features))\n",
    "        \n",
    "        # Track topic statistics\n",
    "        topic_papers = defaultdict(int)  # topic_idx -> count\n",
    "        topic_scores = defaultdict(float)  # topic_idx -> accumulated score\n",
    "        topic_collabs = defaultdict(int)  # topic_idx -> collaboration count\n",
    "        \n",
    "        # Track edge information\n",
    "        edge_dict = {}  # (idx1, idx2) -> {weight, papers, ...}\n",
    "        \n",
    "        # Process papers\n",
    "        paper_count = 0\n",
    "        valid_edges = 0\n",
    "        \n",
    "        for paper in papers:\n",
    "            paper_count += 1\n",
    "            if paper_count % 10000 == 0:\n",
    "                self.logger.info(f\"Processed {paper_count} papers...\")\n",
    "            \n",
    "            # Get valid topics for this paper\n",
    "            topics = [t for t in paper.get('topics', []) \n",
    "                     if t.get('score', 0) >= self.min_topic_score \n",
    "                     and t['id'] in self.topic_to_idx]\n",
    "            \n",
    "            # Update node statistics\n",
    "            for topic in topics:\n",
    "                topic_idx = self.topic_to_idx[topic['id']]\n",
    "                topic_papers[topic_idx] += 1\n",
    "                topic_scores[topic_idx] += topic['score']\n",
    "            \n",
    "            # Process topic pairs to create edges\n",
    "            for i in range(len(topics)):\n",
    "                for j in range(i + 1, len(topics)):\n",
    "                    topic1 = topics[i]\n",
    "                    topic2 = topics[j]\n",
    "                    \n",
    "                    # Get topic indices\n",
    "                    idx1 = self.topic_to_idx[topic1['id']]\n",
    "                    idx2 = self.topic_to_idx[topic2['id']]\n",
    "                    \n",
    "                    # Ensure consistent ordering\n",
    "                    if idx1 > idx2:\n",
    "                        idx1, idx2 = idx2, idx1\n",
    "                        topic1, topic2 = topic2, topic1\n",
    "                    \n",
    "                    # Calculate edge weight\n",
    "                    weight = topic1['score'] * topic2['score']\n",
    "                    \n",
    "                    # Update edge information\n",
    "                    edge_key = (idx1, idx2)\n",
    "                    if edge_key not in edge_dict:\n",
    "                        edge_dict[edge_key] = {\n",
    "                            'weight': weight,\n",
    "                            'papers': {paper['id']},\n",
    "                            'topic1': topic1,\n",
    "                            'topic2': topic2\n",
    "                        }\n",
    "                        valid_edges += 1\n",
    "                    else:\n",
    "                        edge_dict[edge_key]['weight'] += weight\n",
    "                        edge_dict[edge_key]['papers'].add(paper['id'])\n",
    "                    \n",
    "                    # Update collaboration counts\n",
    "                    topic_collabs[idx1] += 1\n",
    "                    topic_collabs[idx2] += 1\n",
    "        \n",
    "        self.logger.info(f\"Found {valid_edges} unique topic connections\")\n",
    "        \n",
    "        # Build node features\n",
    "        self.logger.info(\"Building node features...\")\n",
    "        for topic_idx in range(num_topics):\n",
    "            topic_info = self.idx_to_topic[topic_idx]['info']\n",
    "            papers_count = topic_papers[topic_idx]\n",
    "            \n",
    "            # Basic features [papers, avg_score, total_collabs]\n",
    "            node_features[topic_idx, 0] = papers_count\n",
    "            node_features[topic_idx, 1] = (topic_scores[topic_idx] / papers_count \n",
    "                                         if papers_count > 0 else 0)\n",
    "            node_features[topic_idx, 2] = topic_collabs[topic_idx]\n",
    "            \n",
    "            # One-hot encodings for hierarchical information\n",
    "            domain_name = topic_info['domain']\n",
    "            field_name = topic_info['field']\n",
    "            subfield_name = topic_info['subfield']\n",
    "            \n",
    "            # Add domain one-hot\n",
    "            for domain_id, idx in self.domain_to_idx.items():\n",
    "                if domain_name in domain_id:\n",
    "                    node_features[topic_idx, 3 + idx] = 1\n",
    "                    break\n",
    "            \n",
    "            # Add field one-hot\n",
    "            for field_id, idx in self.field_to_idx.items():\n",
    "                if field_name in field_id:\n",
    "                    node_features[topic_idx, 3 + num_domains + idx] = 1\n",
    "                    break\n",
    "            \n",
    "            # Add subfield one-hot\n",
    "            for subfield_id, idx in self.subfield_to_idx.items():\n",
    "                if subfield_name in subfield_id:\n",
    "                    node_features[topic_idx, 3 + num_domains + num_fields + idx] = 1\n",
    "                    break\n",
    "        \n",
    "        # Build edge index and features\n",
    "        self.logger.info(\"Building edge index and features...\")\n",
    "        edge_index = []\n",
    "        edge_features = []\n",
    "        \n",
    "        for (idx1, idx2), edge_data in edge_dict.items():\n",
    "            edge_index.append([idx1, idx2])\n",
    "            \n",
    "            # Get topic info\n",
    "            topic1_info = self.idx_to_topic[idx1]['info']\n",
    "            topic2_info = self.idx_to_topic[idx2]['info']\n",
    "            \n",
    "            # Calculate edge features\n",
    "            weight = edge_data['weight']\n",
    "            joint_papers = len(edge_data['papers'])\n",
    "            \n",
    "            # Compare hierarchical relationships\n",
    "            same_domain = topic1_info['domain'] == topic2_info['domain']\n",
    "            same_field = topic1_info['field'] == topic2_info['field']\n",
    "            same_subfield = topic1_info['subfield'] == topic2_info['subfield']\n",
    "            \n",
    "            # Normalize weight by number of joint papers\n",
    "            norm_weight = weight / joint_papers if joint_papers > 0 else 0\n",
    "            \n",
    "            edge_features.append([\n",
    "                weight,               # Raw cooperation weight\n",
    "                norm_weight,         # Weight normalized by joint papers\n",
    "                joint_papers,        # Number of papers with both topics\n",
    "                float(same_domain),  # Same domain indicator\n",
    "                float(same_field),   # Same field indicator\n",
    "                float(same_subfield) # Same subfield indicator\n",
    "            ])\n",
    "        \n",
    "        # Convert to numpy arrays\n",
    "        edge_index = np.array(edge_index).T  # Convert to [2 x num_edges] format\n",
    "        edge_features = np.array(edge_features)\n",
    "        \n",
    "        self.logger.info(f\"Created graph with {len(node_features)} nodes and {len(edge_index[0])} edges\")\n",
    "        \n",
    "        # Return complete graph structure\n",
    "        return {\n",
    "                'node_features': node_features,\n",
    "                'edge_index': edge_index,\n",
    "                'edge_features': edge_features,\n",
    "                'num_nodes': num_topics,\n",
    "                'feature_info': {\n",
    "                    'node_feature_dims': total_features,\n",
    "                    'edge_feature_dims': edge_features.shape[1] if len(edge_features) > 0 else 0,\n",
    "                    'node_feature_names': [\n",
    "                        'papers', 'avg_score', 'total_collabs',\n",
    "                        *[f'domain_{i}' for i in range(num_domains)],\n",
    "                        *[f'field_{i}' for i in range(num_fields)],\n",
    "                        *[f'subfield_{i}' for i in range(num_subfields)]\n",
    "                    ],\n",
    "                    'edge_feature_names': [\n",
    "                        'weight', \n",
    "                        'normalized_weight', \n",
    "                        'joint_papers',\n",
    "                        'same_domain', \n",
    "                        'same_field', \n",
    "                        'same_subfield'\n",
    "                    ]\n",
    "                },\n",
    "                'mappings': {\n",
    "                    'topic_to_idx': self.topic_to_idx,\n",
    "                    'domain_to_idx': self.domain_to_idx,\n",
    "                    'field_to_idx': self.field_to_idx,\n",
    "                    'subfield_to_idx': self.subfield_to_idx,\n",
    "                    'idx_to_topic': self.idx_to_topic,\n",
    "                    'idx_to_domain': self.idx_to_domain,\n",
    "                    'idx_to_field': self.idx_to_field,\n",
    "                    'idx_to_subfield': self.idx_to_subfield\n",
    "                }\n",
    "            }\n",
    "\n",
    "    def save_gnn_data(self, gnn_data: Dict, filename: str = 'gnn_data') -> None:\n",
    "        \"\"\"Save GNN data structure to disk and print summary\"\"\"\n",
    "        self.metadata['timestamp'] = datetime.now().isoformat()\n",
    "        \n",
    "        save_data = {\n",
    "            'gnn_data': gnn_data,\n",
    "            'metadata': self.metadata\n",
    "        }\n",
    "        \n",
    "        # Save as both pickle and npz\n",
    "        pickle_path = Path(self.processed_folder) / f'{filename}.pkl'\n",
    "        npz_path = Path(self.processed_folder) / f'{filename}.npz'\n",
    "        \n",
    "        # Save complete data as pickle\n",
    "        with open(pickle_path, 'wb') as f:\n",
    "            pickle.dump(save_data, f)\n",
    "        \n",
    "        # Save numerical data as npz\n",
    "        np.savez(\n",
    "            npz_path,\n",
    "            node_features=gnn_data['node_features'],\n",
    "            edge_index=gnn_data['edge_index'],\n",
    "            edge_features=gnn_data['edge_features']\n",
    "        )\n",
    "        \n",
    "        self.logger.info(f\"Saved GNN data to {pickle_path} and {npz_path}\")\n",
    "        \n",
    "        # Print correct summary\n",
    "        print(\"\\nGNN Data Summary:\")\n",
    "        print(f\"Number of nodes (topics): {gnn_data['num_nodes']}\")\n",
    "        print(f\"Number of edges: {len(gnn_data['edge_index'][0])}\")  # Use edge_index length\n",
    "        print(f\"Node feature dimensions: {gnn_data['feature_info']['node_feature_dims']}\")\n",
    "        print(f\"Edge feature dimensions: {gnn_data['feature_info']['edge_feature_dims']}\")\n",
    "        \n",
    "        # Print additional statistics\n",
    "        print(\"\\nFeature Breakdowns:\")\n",
    "        print(\"Node features:\")\n",
    "        for i, name in enumerate(gnn_data['feature_info']['node_feature_names'][:3]):\n",
    "            print(f\"- {name}: mean = {np.mean(gnn_data['node_features'][:, i]):.2f}\")\n",
    "        \n",
    "        print(\"\\nEdge features:\")\n",
    "        for i, name in enumerate(gnn_data['feature_info']['edge_feature_names']):\n",
    "            print(f\"- {name}: mean = {np.mean(gnn_data['edge_features'][:, i]):.2f}\")\n",
    "\n",
    "    def load_gnn_data(self, filename: str = 'gnn_data') -> Optional[Dict]:\n",
    "        \"\"\"Load previously saved GNN data\"\"\"\n",
    "        pickle_path = Path(self.processed_folder) / f'{filename}.pkl'\n",
    "        \n",
    "        if pickle_path.exists():\n",
    "            try:\n",
    "                with open(pickle_path, 'rb') as f:\n",
    "                    data = pickle.load(f)\n",
    "                \n",
    "                if data['metadata']['min_topic_score'] == self.min_topic_score:\n",
    "                    self.logger.info(f\"Loaded GNN data from {pickle_path}\")\n",
    "                    return data['gnn_data']\n",
    "                \n",
    "            except Exception as e:\n",
    "                self.logger.error(f\"Error loading GNN data: {str(e)}\")\n",
    "        \n",
    "        return None\n",
    "\n",
    "    \n",
    "def load_papers(data_folder: str) -> List[dict]:\n",
    "    \"\"\"Load papers from JSON files\"\"\"\n",
    "    pattern = os.path.join(data_folder, \"highly_cited_articles_2024*.json\")\n",
    "    files = sorted(glob.glob(pattern))\n",
    "    \n",
    "    if not files:\n",
    "        raise FileNotFoundError(f\"No JSON files found in {data_folder}\")\n",
    "    \n",
    "    all_papers = []\n",
    "    for file_path in files:\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                # Check if this is a single paper or list of papers\n",
    "                data = json.load(f)\n",
    "                if isinstance(data, dict):\n",
    "                    all_papers.append(data)\n",
    "                elif isinstance(data, list):\n",
    "                    all_papers.extend(data)\n",
    "                else:\n",
    "                    logging.warning(f\"Unexpected data format in {file_path}\")\n",
    "                    \n",
    "                logging.info(f\"Loaded {len(all_papers)} papers from {file_path}\")\n",
    "        except json.JSONDecodeError as e:\n",
    "            logging.error(f\"Error decoding JSON from {file_path}: {str(e)}\")\n",
    "            continue\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error reading {file_path}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    logging.info(f\"Total papers loaded: {len(all_papers)}\")\n",
    "    return all_papers\n",
    "    \n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "    \n",
    "    try:\n",
    "        # Initialize builder\n",
    "        gnn_builder = GNNDataBuilder(min_topic_score=0.3)\n",
    "        \n",
    "        # Load papers\n",
    "        papers = load_papers(\"openalex_topics_results\")\n",
    "        \n",
    "        # Build indices\n",
    "        gnn_builder.build_category_indices(papers)\n",
    "        \n",
    "        # Build GNN data\n",
    "        gnn_data = gnn_builder.build_gnn_data(papers)\n",
    "        \n",
    "        # Save data\n",
    "        gnn_builder.save_gnn_data(gnn_data)\n",
    "        \n",
    "        # Print summary\n",
    "        print(\"\\nGNN Data Summary:\")\n",
    "        print(f\"Number of nodes (topics): {gnn_data['num_nodes']}\")\n",
    "        print(f\"Number of edges: {len(gnn_data['edge_index'])}\")\n",
    "        print(f\"Node feature dimensions: {gnn_data['feature_info']['node_feature_dims']}\")\n",
    "        print(f\"Edge feature dimensions: {gnn_data['feature_info']['edge_feature_dims']}\")\n",
    "\n",
    "\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in processing: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f396f612-a597-4a56-9a94-3aad6ad9bd94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Graph Structure Analysis:\n",
      "Total nodes: 3402\n",
      "Total edges: 30143\n",
      "\n",
      "Top 5 Most Connected Topics:\n",
      "- Gut microbiota and health\n",
      "  Connections: 158\n",
      "  Field: Biochemistry, Genetics and Molecular Biology\n",
      "  Subfield: Molecular Biology\n",
      "- Artificial Intelligence in Healthcare and Education\n",
      "  Connections: 152\n",
      "  Field: Medicine\n",
      "  Subfield: Health Informatics\n",
      "- Machine Learning in Materials Science\n",
      "  Connections: 141\n",
      "  Field: Materials Science\n",
      "  Subfield: Materials Chemistry\n",
      "- Epigenetics and DNA Methylation\n",
      "  Connections: 129\n",
      "  Field: Biochemistry, Genetics and Molecular Biology\n",
      "  Subfield: Molecular Biology\n",
      "- Single-cell and spatial transcriptomics\n",
      "  Connections: 121\n",
      "  Field: Biochemistry, Genetics and Molecular Biology\n",
      "  Subfield: Molecular Biology\n",
      "\n",
      "Top 5 Strongest Topic Collaborations:\n",
      "- Advancements in Battery Materials <-> Advanced Battery Materials and Technologies\n",
      "  Weight: 3999.097\n",
      "  Joint Papers: 239.0\n",
      "  Fields: Engineering <-> Engineering\n",
      "- Advancements in Battery Materials <-> Advanced Battery Technologies Research\n",
      "  Weight: 2358.918\n",
      "  Joint Papers: 148.0\n",
      "  Fields: Engineering <-> Engineering\n",
      "- Advanced Battery Materials and Technologies <-> Advanced Battery Technologies Research\n",
      "  Weight: 2192.910\n",
      "  Joint Papers: 132.0\n",
      "  Fields: Engineering <-> Engineering\n",
      "- Quantum Information and Cryptography <-> Quantum Computing Algorithms and Architecture\n",
      "  Weight: 1948.060\n",
      "  Joint Papers: 128.0\n",
      "  Fields: Computer Science <-> Computer Science\n",
      "- Galaxies: Formation, Evolution, Phenomena <-> Astronomy and Astrophysical Research\n",
      "  Weight: 1816.854\n",
      "  Joint Papers: 107.0\n",
      "  Fields: Physics and Astronomy <-> Physics and Astronomy\n",
      "\n",
      "Top 5 Field-Level Collaborations:\n",
      "- Biochemistry, Genetics and Molecular Biology <-> Medicine: 2049 connections\n",
      "- Engineering <-> Materials Science: 839 connections\n",
      "- Computer Science <-> Engineering: 610 connections\n",
      "- Immunology and Microbiology <-> Medicine: 547 connections\n",
      "- Medicine <-> Neuroscience: 516 connections\n"
     ]
    }
   ],
   "source": [
    "def analyze_graph_structure(gnn_data: Dict):\n",
    "        \"\"\"\n",
    "        Analyze the graph structure to verify edges and show key insights\n",
    "        \"\"\"\n",
    "        node_features = gnn_data['node_features']\n",
    "        edge_index = gnn_data['edge_index']\n",
    "        edge_features = gnn_data['edge_features']\n",
    "        mappings = gnn_data['mappings']\n",
    "        \n",
    "        print(\"\\nGraph Structure Analysis:\")\n",
    "        print(f\"Total nodes: {gnn_data['num_nodes']}\")\n",
    "        print(f\"Total edges: {len(edge_index[0])}\")\n",
    "        \n",
    "        # 1. Analyze node connectivity\n",
    "        node_connections = defaultdict(int)\n",
    "        for source, target in edge_index.T:  # Transpose to get [source, target] pairs\n",
    "            node_connections[source] += 1\n",
    "            node_connections[target] += 1\n",
    "        \n",
    "        # Find most connected topics\n",
    "        most_connected = sorted(node_connections.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "        print(\"\\nTop 5 Most Connected Topics:\")\n",
    "        for node_idx, connection_count in most_connected:\n",
    "            topic_info = mappings['idx_to_topic'][node_idx]['info']\n",
    "            print(f\"- {topic_info['display_name']}\")\n",
    "            print(f\"  Connections: {connection_count}\")\n",
    "            print(f\"  Field: {topic_info['field']}\")\n",
    "            print(f\"  Subfield: {topic_info['subfield']}\")\n",
    "        \n",
    "        # 2. Analyze strongest collaborations\n",
    "        edge_strengths = []\n",
    "        for i, (source, target) in enumerate(edge_index.T):\n",
    "            source_info = mappings['idx_to_topic'][source]['info']\n",
    "            target_info = mappings['idx_to_topic'][target]['info']\n",
    "            \n",
    "            edge_strengths.append({\n",
    "                'source': source_info['display_name'],\n",
    "                'target': target_info['display_name'],\n",
    "                'weight': edge_features[i][0],  # Raw weight\n",
    "                'joint_papers': edge_features[i][2],  # Number of joint papers\n",
    "                'same_field': bool(edge_features[i][4]),  # Same field indicator\n",
    "                'fields': (source_info['field'], target_info['field'])\n",
    "            })\n",
    "        \n",
    "        # Sort by weight\n",
    "        strongest_edges = sorted(edge_strengths, key=lambda x: x['weight'], reverse=True)[:5]\n",
    "        print(\"\\nTop 5 Strongest Topic Collaborations:\")\n",
    "        for edge in strongest_edges:\n",
    "            print(f\"- {edge['source']} <-> {edge['target']}\")\n",
    "            print(f\"  Weight: {edge['weight']:.3f}\")\n",
    "            print(f\"  Joint Papers: {edge['joint_papers']}\")\n",
    "            print(f\"  Fields: {edge['fields'][0]} <-> {edge['fields'][1]}\")\n",
    "        \n",
    "        # 3. Analyze field-level connections\n",
    "        field_connections = defaultdict(lambda: defaultdict(int))\n",
    "        for i, (source, target) in enumerate(edge_index.T):\n",
    "            source_field = mappings['idx_to_topic'][source]['info']['field']\n",
    "            target_field = mappings['idx_to_topic'][target]['info']['field']\n",
    "            if source_field != target_field:\n",
    "                if source_field > target_field:\n",
    "                    source_field, target_field = target_field, source_field\n",
    "                field_connections[source_field][target_field] += 1\n",
    "        \n",
    "        print(\"\\nTop 5 Field-Level Collaborations:\")\n",
    "        field_pairs = [(f1, f2, count) \n",
    "                      for f1, connections in field_connections.items() \n",
    "                      for f2, count in connections.items()]\n",
    "        top_field_pairs = sorted(field_pairs, key=lambda x: x[2], reverse=True)[:5]\n",
    "        \n",
    "        for field1, field2, count in top_field_pairs:\n",
    "            print(f\"- {field1} <-> {field2}: {count} connections\")\n",
    "    \n",
    "# Usage\n",
    "with open('processed_data/gnn_data.pkl', 'rb') as f:\n",
    "    saved_data = pickle.load(f)\n",
    "    gnn_data = saved_data['gnn_data']\n",
    "\n",
    "analyze_graph_structure(gnn_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c02e955-4d62-483a-ade7-04aebee928f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gnn2025",
   "language": "python",
   "name": "gnn2025"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
